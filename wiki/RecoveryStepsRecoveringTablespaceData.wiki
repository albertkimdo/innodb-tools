#summary Recovery steps: recovering tablespace data

== Building the code ==

As explained earlier, we must build the application against the generate table definition.
We do so with `make`:

{{{
[root@test innodb-recovery-0.3]# make
gcc -DHAVE_OFFSET64_T -D_FILE_OFFSET_BITS=64 -D_LARGEFILE64_SOURCE=1 -D_LARGEFILE_SOURCE=1 -g -I include -I mysql-source/include -I mysql-source/innobase/include -c tables_dict.c -o lib/tables_dict.o
gcc -DHAVE_OFFSET64_T -D_FILE_OFFSET_BITS=64 -D_LARGEFILE64_SOURCE=1 -D_LARGEFILE_SOURCE=1 -g -I include -I mysql-source/include -I mysql-source/innobase/include -o constraints_parser constraints_parser.c lib/tables_dict.o lib/print_data.o lib/check_data.o lib/libut.a lib/libmystrings.a
gcc -DHAVE_OFFSET64_T -D_FILE_OFFSET_BITS=64 -D_LARGEFILE64_SOURCE=1 -D_LARGEFILE_SOURCE=1 -g -I include -I mysql-source/include -I mysql-source/innobase/include -o page_parser page_parser.c lib/tables_dict.o lib/libut.a
}}}

We should now have the `constraints_parser` application built and ready to run.
{{{
[root@test innodb-recovery-0.3]# ls -l constraints_parser
-rwxr-xr-x 1 root root 690262 Jun 30 05:28 constraints_parser
}}}


== Executing the constraints_parser ==

The following step is the core of the recovery process: attempting to resolve some sensible data from the tablespace. We do so by running the `constraints_parser` against our tablespace file.

  * If you were using innodb_file_per_table, then this would be the .ibd file. 
  * Otherwise this is the merged tablespace file, the result of the last few steps.

In our example, this is:
{{{
[root@test innodb-recovery-0.3]# ./constraints_parser -5 -f /tmp/customer_data 
}}}

Now here comes the flood. The output of this run should most probably provide with quite more than is required. Too see how, we look at the example:

{{{
customer        0       120     ""      ""      ""      32770   0       "0000-00-00 00:12:80"   0
customer        0       0       ""      ""      ""      0       0       "9120-22-48 29:44:00"   2
customer        61953   0       ""      ""      ""      2816    0       "7952-32-67 11:43:49"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
...
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "2562-67-51 40:71:15"   482288752
customer        0       0       ""      ""      ""      1280    0       "3274-79-03 95:90:40"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   16777728
customer        28262   114     ""      ""      NULL    25965   117     "4603-91-96 76:21:28"   5111809
customer        0       82      ""      ""      ""      22867   77      "2775-94-58 03:19:18"   1397573972
customer        2       1       "PATRICIA"      "JOHNSON"       "PATRICIA.JOHNSON@sakilacustomer.org"   6       1       "2006-02-14 22:04:36"   1140008240
customer        3       1       "LINDA" "WILLIAMS"      "LINDA.WILLIAMS@sakilacustomer.org"     7       1       "2006-02-14 22:04:36"   1140008240
customer        4       2       "BARBARA"       "JONES" "BARBARA.JONES@sakilacustomer.org"      8       1       "2006-02-14 22:04:36"   1140008240
customer        5       1       "ELIZABETH"     "BROWN" "ELIZABETH.BROWN@sakilacustomer.org"    9       1       "2006-02-14 22:04:36"   1140008240
customer        6       2       "JENNIFER"      "DAVIS" "JENNIFER.DAVIS@sakilacustomer.org"     10      1       "2006-02-14 22:04:36"   1140008240
customer        7       1       "MARIA" "MILLER"        "MARIA.MILLER@sakilacustomer.org"       11      1       "2006-02-14 22:04:36"   1140008240
customer        8       2       "SUSAN" "WILSON"        "SUSAN.WILSON@sakilacustomer.org"       12      1       "2006-02-14 22:04:36"   1140008240
customer        9       2       "MARGARET"      "MOORE" "MARGARET.MOORE@sakilacustomer.org"     13      1       "2006-02-14 22:04:36"   1140008240
...
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "0000-00-00 00:00:00"   0
customer        0       0       ""      ""      ""      0       0       "7679-35-98 86:44:53"   720578985
}}}

Some of this data is good; some is obviously garbage. And some should take careful attention.

== Revisiting table_defs.h ==

To try and make sense into the result, we can set constraints on the data. This is done by editing the `include/table_defs.h` file.

Some values above are clearly wrong: knowing our data, we can conclude there's no customer with customer_id `0`, nor is there a customer with an empty `first_name`, etc.

The constraints can be edited, within the `limits {}` clause, per table column. For example:

{{{
	{ /* smallint(5) unsigned */
		name: "customer_id",
		type: FT_UINT,
		fixed_length: 2,

		has_limits: TRUE,
		limits: {
		        can_be_null: FALSE,
		        uint_min_val: 0,
		        uint_max_val: 65535
		},

		can_be_null: FALSE
	},
}}}

Can be edited into:
{{{
	{ /* smallint(5) unsigned */
		name: "customer_id",
		type: FT_UINT,
		fixed_length: 2,

		has_limits: TRUE,
		limits: {
		        can_be_null: FALSE,
		        uint_min_val: 1,
		        uint_max_val: 65535
		},

		can_be_null: FALSE
	},
}}}

(Adding the constraint that customer_id must be greater than, or equal, to `1`).
Also:
{{{
        { /* varchar(45) */
                name: "first_name",
                type: FT_CHAR,
                min_length: 0,
                max_length: 45,

                has_limits: TRUE,
                limits: {
                        can_be_null: FALSE,
                        char_min_len: 0,
                        char_max_len: 45,
                        char_ascii_only: TRUE
                },

                can_be_null: FALSE
        },
}}}

Can be changed to:
{{{
        { /* varchar(45) */
                name: "first_name",
                type: FT_CHAR,
                min_length: 0,
                max_length: 45,

                has_limits: TRUE,
                limits: {
                        can_be_null: FALSE,
                        char_min_len: 1,
                        char_max_len: 45,
                        char_ascii_only: TRUE
                },

                can_be_null: FALSE
        },
}}}

See [ConstraintsTypes Types of constraints] for detailed listing of constraints.

This last change may not be strict enough. Can there really be a customer with a one-letter first name? The better you know your data, the tighter the constraints you can place, and the better results to get from `constraints_parser`.

It is necessary to run `constraints_parser` again.


== Re-executing the constraints_parser ==

The process repeats itself. We edit the `table_defs.h` file, rebuild, and run the parser again.

In our example:
{{{
[root@test innodb-recovery-0.3]# make
gcc -DHAVE_OFFSET64_T -D_FILE_OFFSET_BITS=64 -D_LARGEFILE64_SOURCE=1 -D_LARGEFILE_SOURCE=1 -g -I include -I mysql-source/include -I mysql-source/innobase/include -c tables_dict.c -o lib/tables_dict.o
gcc -DHAVE_OFFSET64_T -D_FILE_OFFSET_BITS=64 -D_LARGEFILE64_SOURCE=1 -D_LARGEFILE_SOURCE=1 -g -I include -I mysql-source/include -I mysql-source/innobase/include -c print_data.c -o lib/print_data.o 
print_data.c:217:2: warning: no newline at end of file
gcc -DHAVE_OFFSET64_T -D_FILE_OFFSET_BITS=64 -D_LARGEFILE64_SOURCE=1 -D_LARGEFILE_SOURCE=1 -g -I include -I mysql-source/include -I mysql-source/innobase/include -c check_data.c -o lib/check_data.o
gcc -DHAVE_OFFSET64_T -D_FILE_OFFSET_BITS=64 -D_LARGEFILE64_SOURCE=1 -D_LARGEFILE_SOURCE=1 -g -I include -I mysql-source/include -I mysql-source/innobase/include -o constraints_parser constraints_parser.c lib/tables_dict.o lib/print_data.o lib/check_data.o lib/libut.a lib/libmystrings.a
gcc -DHAVE_OFFSET64_T -D_FILE_OFFSET_BITS=64 -D_LARGEFILE64_SOURCE=1 -D_LARGEFILE_SOURCE=1 -g -I include -I mysql-source/include -I mysql-source/innobase/include -o page_parser page_parser.c lib/tables_dict.o lib/libut.a
}}}

Then:
{{{
[root@test innodb-recovery-0.3]# ./constraints_parser -5 -f /tmp/customer_data 
}}}

In our example, we actually ran:
{{{
[root@test innodb-recovery-0.3]# ./constraints_parser -5 -f /tmp/customer_data > /tmp/customer_data.csv
}}}

Looking at the result file, we find:
{{{
customer        1       1       "MARY"  "SMITH" "MARY.SMITH@sakilacustomer.org" 5       1       "2006-02-14 22:04:36"   1140008240
customer        2       1       "PATRICIA"      "JOHNSON"       "PATRICIA.JOHNSON@sakilacustomer.org"   6       1       "2006-02-14 22:04:36"   1140008240
customer        3       1       "LINDA" "WILLIAMS"      "LINDA.WILLIAMS@sakilacustomer.org"     7       1       "2006-02-14 22:04:36"   1140008240
customer        4       2       "BARBARA"       "JONES" "BARBARA.JONES@sakilacustomer.org"      8       1       "2006-02-14 22:04:36"   1140008240
customer        5       1       "ELIZABETH"     "BROWN" "ELIZABETH.BROWN@sakilacustomer.org"    9       1       "2006-02-14 22:04:36"   1140008240
customer        6       2       "JENNIFER"      "DAVIS" "JENNIFER.DAVIS@sakilacustomer.org"     10      1       "2006-02-14 22:04:36"   1140008240
customer        7       1       "MARIA" "MILLER"        "MARIA.MILLER@sakilacustomer.org"       11      1       "2006-02-14 22:04:36"   1140008240
customer        8       2       "SUSAN" "WILSON"        "SUSAN.WILSON@sakilacustomer.org"       12      1       "2006-02-14 22:04:36"   1140008240
customer        9       2       "MARGARET"      "MOORE" "MARGARET.MOORE@sakilacustomer.org"     13      1       "2006-02-14 22:04:36"   1140008240
customer        10      1       "DOROTHY"       "TAYLOR"        "DOROTHY.TAYLOR@sakilacustomer.org"     14      1       "2006-02-14 22:04:36"   1140008240
customer        11      2       "LISA"  "ANDERSON"      "LISA.ANDERSON@sakilacustomer.org"      15      1       "2006-02-14 22:04:36"   1140008240
customer        12      1       "NANCY" "THOMAS"        "NANCY.THOMAS@sakilacustomer.org"       16      1       "2006-02-14 22:04:36"   1140008240
...
customer        596     1       "ENRIQUE"       "FORSYTHE"      "ENRIQUE.FORSYTHE@sakilacustomer.org"   602     1       "2006-02-14 22:04:37"   1140008240
customer        597     1       "FREDDIE"       "DUGGAN"        "FREDDIE.DUGGAN@sakilacustomer.org"     603     1       "2006-02-14 22:04:37"   1140008240
customer        598     1       "WADE"  "DELVALLE"      "WADE.DELVALLE@sakilacustomer.org"      604     1       "2006-02-14 22:04:37"   1140008240
customer        599     2       "AUSTIN"        "CINTRON"       "AUSTIN.CINTRON@sakilacustomer.org"     605     1       "2006-02-14 22:04:37"   1140008240
}}}

It looks like all rows have been recovered correctly.

In other cases, an `UPDATE` may have been performed on the table, prior to damage, which would result in two rows with the same customer_id. We have created the merged file in such way that later updates appear later in the file. But should you want to recover from that `UPDATE`, it is up to your understanding of your data to know which of the two rows (or more) is to be removed.

Rows that have been `DELETE`d may still appear in the result. Run `constraints_parser` with the *-D* option to only view deleted rows. then manually (or build your own script) remove them from the results.

== Results ==

We are now left with a `CSV` file.

Next: [RecoveryStepsImportDataIntoTable importing tab-delimited data into table]